# Proprosal 

## Collaborators

Our group members are Teo Zeng, Phakphum Artkaew, Naman Soni, Zhixiang Zhou, and Yichen Shi.

## Research Question

The interplay between music and human emotions is a profound and well-documented phenomenon. On one hand, music has the power to evoke and amplify a wide range of emotional responses, from joy to sorrow and from tranquility to agitation. On the other hand, advancements in artificial intelligence in recent years have paved the way for exploring new dimensions of this interaction, especially with the advent of Large Language Models (LLMs). This class project delves into the capabilities of LLMs in interpreting textual inputs that convey human emotional states and translating these inputs into musical compositions. W**e therefore hypothesize that LLMs, through their processing of textual inputs indicative of human emotional states, can generate musical compositions that are perceived by human listeners as embodying these specific emotions. Furthermore, we propose that the quality of the music generated from our framework, which utilizes a fine-tuned BART model, may potentially surpass the quality of music directly generated by prompting a GPT-4 - similar model to create MIDI files**. To evaluate the effectiveness of our approach, we are also proposing a framework to assess the emotional content underlying the generated music files, enabling us to compare the quality of music produced by our model against other methods. We aim to explore the potential of LLMs in bridging the realms of language and music, thereby offering innovative insights into the synthesis of artificial intelligence, emotional expression, and musical creativity.

## Background/Literature Search

So far, we have identified the following relevant papers for our project.

- [Hung et al. (2021)](https://arxiv.org/abs/2108.01374) introduce the EMOPIA dataset, which contains piano MIDI files and their corresponding emotional labels, such as low valence, high valence, high arousal, and low arousal.
- [Lam et al. (2024)](https://papers.nips.cc/paper_files/paper/2023/hash/38b23e2328096520e9c889ae03e372c9-Abstract-Conference.html) introduce MeLoDy, an LM-guided diffusion model that efficiently generates high-quality music. MeLoDy inherits MusicLM's semantic LM and employs a novel dual-path diffusion model with an audio VAE-GAN to decode semantic tokens into waveform. This reduces 95.7-99.6% of MusicLM's forward passes for 10-30s music generation while achieving state-of-the-art musicality, audio quality, and text correlation.
- [Mihalcea et al. (2012)](https://aclanthology.org/D12-1054/) introduce a corpus containing lyrics and music from 100 songs, with each song annotated to indicate the presence of emotions such as anger, disgust, fear, joy, sadness, and surprise.
- [Zeng et al. (2021)](https://aclanthology.org/2021.findings-acl.70/) propose MusicBERT, a large pre-trained model for symbolic music understanding, leveraging a 1M+ song corpus and novel pretraining techniques like OctupleMIDI encoding and bar-level masking, demonstrating improved performance on various music tasks.
- [Yu et al. (2022)](https://arxiv.org/pdf/2210.10349v2.pdf) propose Museformer, an advanced transformer-based model designed for symbolic music understanding and generation, trained on a large MIDI dataset, showcasing superior performance in music completion, style transfer, and accompaniment generation tasks.
- [Doh et al. (2020)](https://arxiv.org/pdf/2008.01190v1.pdf) applies word embedding techniques to music information retrieval tasks. By training distributed representation of words using both general and music-specific text data, the research evaluates how well these embeddings assoSciate listening contexts with musical compositions.

## Methology Proposal

### Dataset

The goal of this study is to investigate whether the GPT-2 language model can be fine-tuned to generate MIDI files from human text prompts. To create a dataset for this task, we will leverage the existing EMOPIA dataset of labeled MIDI files and generate additional prompt-MIDI pairs using GPT-4.

The EMOPIA dataset contains around 300 MIDI files labeled with valence (high or low) and arousal (high or low) values. We will use the GPT-4 language model to generate text prompts along with corresponding valence and arousal labels for new MIDI files. Specifically: We will provide examples to GPT-4 in the format:

```
Prompt: "Generate a serene and peaceful musical piece that evokes a sense of calmness and relaxation."

Label: Low valence, Low arousal
```

Given this template, GPT-4 will generate new text prompts describing desired musical characteristics, along with a predicted valence and arousal label for each prompt. The generated prompts and labels will be manually reviewed to filter out any low-quality or inconsistent examples. For each validated prompt-label pair, we will use GPT-4's capabilities to generate a corresponding MIDI file output. The new prompt-label-MIDI tuples will be combined with the original EMOPIA data to create the final training dataset.

### Model

Our proposed model leverages the power of the BART (Bidirectional and Auto-Regressive Transformers) architecture. To adapt BART for our specific task of generating music from textual prompts, we employ a two-step tokenization process.

First, we utilize the BART tokenizer to tokenize the textual prompts from our dataset. The BART tokenizer is a subword tokenizer that uses byte-pair encoding (BPE) to break down words into subword units, allowing the model to handle out-of-vocabulary words effectively. By tokenizing the prompts with the BART tokenizer, we ensure that the input text is transformed into a suitable format for the BART model to process.

Second, we develop our own custom tokenizer to tokenize the MIDI files in ABC notation. By converting the MIDI files to ABC notation and tokenizing them with our custom tokenizer, we create a unified representation of the musical data that can be easily integrated with the BART model.

To train our model, we fine-tune the pre-trained BART base model using our dataset of prompt-MIDI pairs. The BART base model, which has been pre-trained on a large corpus of text data, serves as a strong foundation for our task. During the fine-tuning process, we feed the tokenized prompts as input to the BART encoder and the corresponding tokenized MIDI files as the target output. The model learns to map the emotional content of the prompts to the appropriate musical elements in the MIDI files.

### Evaluation

We will first ensure that the midi notation generated by our BART model is machine-playable. To investigate the underlying emotion states, we will further evaluate our model output in the following experiments:

#### Experiment 1: Evaluating Emotional Coherence between Prompts and Generated Music

To evaluate the emotional coherence between the textual prompts and the generated MIDI music, we propose a two-step evaluation process involving two separate BERT models.

##### Model 1: Prompt Emotion Classification

We train a BERT-based model, denoted as Model 1, to classify the emotional content of the textual prompts. The input to Model 1 is a prompt $x$, and the output is a pair of valence and arousal scores $(v_x, a_x)$, where $v_x, a_x \in [0, 1]$. Model 1 is trained using our dataset, where each prompt is associated with a categorical label of (high, low) valence × (high, low) arousal. The valence and arousal scores are obtained by mapping the categorical labels to numerical values.

##### Model 2: MIDI Emotion Classification

Similarly, we train another BERT-based model, denoted as Model 2, to classify the emotional content of MIDI files in ABC notation. The input to Model 2 is a MIDI file $y$ in ABC notation, and the output is a pair of valence and arousal scores $(v_y, a_y)$, where $v_y, a_y \in [0, 1]$. Model 2 is trained using our dataset, where each MIDI file has an associated (high, low) valence × (high, low) arousal label, directly obtained from the EMOPIA dataset.

To evaluate the emotional coherence between the prompts and the generated music, we perform the following steps:

1. Feed a set of prompts $X = \{x_1, x_2, \dots, x_n\}$ to our trained music generation model, which outputs a corresponding set of MIDI files in ABC notation $Y = \{y_1, y_2, \dots, y_n\}$.
2. Feed the same set of prompts $X$ to Model 1, which outputs a set of valence and arousal scores $\{(v_{x_1}, a_{x_1}), (v_{x_2}, a_{x_2}), \dots, (v_{x_n}, a_{x_n})\}$.
3. Feed the generated MIDI files $Y$ to Model 2, which outputs a set of valence and arousal scores $\{(v_{y_1}, a_{y_1}), (v_{y_2}, a_{y_2}), \dots, (v_{y_n}, a_{y_n})\}$.
4. Compute the distance between the valence and arousal scores obtained from Model 1 and Model 2 for each prompt-MIDI pair. We use the Euclidean distance as the distance metric:

  $d_i = \sqrt{(v_{x_i} - v_{y_i})^2 + (a_{x_i} - a_{y_i})^2}$

  where $d_i$ represents the emotional coherence distance for the $i$-th prompt-MIDI pair.

5. Evaluate the overall emotional coherence by averaging the distances across all prompt-MIDI pairs:

  $D = \frac{1}{n} \sum_{i=1}^{n} d_i$

 where $D$ represents the average emotional coherence distance.

A lower value of $D$ indicates better emotional coherence between the prompts and the generated music, suggesting that our music generation model effectively captures and translates the emotional content from the textual prompts to the generated MIDI music. By conducting this evaluation experiment, we aim to quantitatively assess the emotional understanding and generation capabilities of our proposed model, providing insights into its effectiveness in creating emotionally coherent music from textual prompts.

#### Experiment 2: Analyzing Model Performance on Specific Prompts

Building upon the evaluation framework established in Experiment 1, we aim to gain deeper insights into the performance of our proposed model and other language models by visually examining their effectiveness on specific prompts. The goal is to identify patterns and characteristics of prompts that pose challenges or are handled well by the models.

##### Prompt Selection
We select a subset of prompts $X' = \{x'_1, x'_2, \dots, x'_m\}$ from the original set of prompts $X$, where $m < n$. The selection criteria for $X'$ can be based on various factors, such as the emotional diversity of the prompts, the complexity of the language used, or the presence of specific themes or genres.

##### Model Comparison
We compare the performance of our proposed model with other language models, such as GPT-4, on the selected prompts $X'$. For each model $k$, we generate a corresponding set of MIDI files $Y'_k = \{y'_{k,1}, y'_{k,2}, \dots, y'_{k,m}\}$ using the selected prompts $X'$.

##### Evaluation Procedure
1. For each prompt $x'_i$ in $X'$, we compute the emotional coherence distance $d_{k,i}$ between the prompt and the generated MIDI file $y'_{k,i}$ for each model $k$, using the evaluation procedure described in Experiment 1.

2. We create a table to visualize the performance of each model on the selected prompts. The table will have the following structure:

   | Prompt | Model 1 Distance | Model 2 Distance | ...  | Model $k$ Distance |
   | ------ | ---------------- | ---------------- | ---- | ------------------ |
   | $x'_1$ | $d_{1,1}$        | $d_{2,1}$        | ...  | $d_{k,1}$          |
   | $x'_2$ | $d_{1,2}$        | $d_{2,2}$        | ...  | $d_{k,2}$          |
   | ...    | ...              | ...              | ...  | ...                |
   | $x'_m$ | $d_{1,m}$        | $d_{2,m}$        | ...  | $d_{k,m}$          |

3. We analyze the results by examining the table and identifying common patterns or characteristics among the prompts where models performed poorly (i.e., high emotional coherence distance) or excelled (i.e., low emotional coherence distance). This analysis may involve considering factors such as the complexity of the language used, the presence of specific emotions or themes, or the length of the prompts.

## Required Resources

We will need computing power to fine-tune BART. We will use NYU HPC for this purpose. Additionally, for human evaluation, we will sample the results and conduct evaluations among ourselves and friends.

## References

Hung, H. T., Ching, J., Doh, S., Kim, N., Nam, J., & Yang, Y. H. (2021). EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation. arXiv preprint arXiv:2108.01374.

Lam, M. W., Tian, Q., Li, T., Yin, Z., Feng, S., Tu, M., ... & Wang, Y. (2024). Efficient neural music generation. Advances in Neural Information Processing Systems, 36.

Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. 2021. MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 791–800, Online. Association for Computational Linguistics.

Rada Mihalcea and Carlo Strapparava. 2012. Lyrics, Music, and Emotions. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea. Association for Computational Linguistics.

Yu, B., Lu, P., Wang, R., Hu, W., Tan, X., Ye, W., Zhang, S., Qin, T., & Liu, T.-Y. (2022). Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation. In Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv preprint arXiv:2210.10349.

Doh, S., Lee, J., Park, T. H., & Nam, J. (2020). Musical Word Embedding: Bridging the Gap between Listening Contexts and Music. In Machine Learning for Media Discovery Workshop, International Conference on Machine Learning (ICML), 2020. arXiv:2008.01190 [cs.IR].